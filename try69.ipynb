{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5950eb38-dca8-4871-9742-0791e6da79bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.171  Python-3.11.5 torch-2.0.1+cpu CPU (Intel Core(TM) i5-10300H 2.50GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=E:\\Python codes\\Learn\\OpenCV\\Interlinked\\Interlinked.v1i.yolov8\\data1.yaml, epochs=20, patience=50, batch=5, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train18\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011628 parameters, 3011612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train18', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning E:\\Python codes\\Learn\\OpenCV\\Interlinked\\Interlinked.v1i.yolov8\\train\\labels.cache... 121 images, 4 bac\u001b[0m\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 89, len(boxes) = 139. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python codes\\Learn\\OpenCV\\Interlinked\\Interlinked.v1i.yolov8\\valid\\labels.cache... 36 images, 0 backgr\u001b[0m\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 25, len(boxes) = 40. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "Plotting labels to runs\\detect\\train18\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005078125), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train18\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/20         0G      1.326      3.684      1.609         11        640: 100%|██████████| 25/25 [01:19<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:12<0\n",
      "                   all         36         40    0.00573      0.908      0.256      0.134\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/20         0G      1.373      3.183      1.674         13        640: 100%|██████████| 25/25 [01:22<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:09<0\n",
      "                   all         36         40      0.344      0.221      0.296      0.162\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/20         0G      1.414      2.879      1.658         17        640: 100%|██████████| 25/25 [01:17<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:09<0\n",
      "                   all         36         40      0.386      0.417      0.356      0.163\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/20         0G      1.382      2.852      1.662          7        640: 100%|██████████| 25/25 [01:21<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:10<0\n",
      "                   all         36         40      0.625      0.462      0.411      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/20         0G      1.386       2.87      1.677          6        640: 100%|██████████| 25/25 [01:28<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:09<0\n",
      "                   all         36         40       0.29       0.59      0.365      0.173\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/20         0G      1.415      2.678       1.62         18        640: 100%|██████████| 25/25 [01:29<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:12<0\n",
      "                   all         36         40      0.341      0.455      0.374      0.202\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/20         0G      1.309       2.43      1.526          7        640: 100%|██████████| 25/25 [01:32<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:11<0\n",
      "                   all         36         40      0.252      0.571      0.366      0.196\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/20         0G      1.365       2.53      1.594          5        640: 100%|██████████| 25/25 [01:42<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:10<0\n",
      "                   all         36         40      0.484      0.478      0.413       0.23\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/20         0G      1.436      2.561      1.691         11        640: 100%|██████████| 25/25 [01:41<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:12<0\n",
      "                   all         36         40      0.416       0.48      0.461      0.248\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/20         0G      1.416      2.442      1.672          8        640: 100%|██████████| 25/25 [01:31<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:10<0\n",
      "                   all         36         40      0.422      0.474      0.468       0.25\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/20         0G      1.353      2.826      1.698          5        640: 100%|██████████| 25/25 [01:29<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:11<0\n",
      "                   all         36         40      0.328      0.453      0.334      0.135\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/20         0G      1.388      2.917      1.767          5        640: 100%|██████████| 25/25 [01:55<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:09<0\n",
      "                   all         36         40      0.366      0.536      0.353      0.146\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/20         0G      1.412      2.832      1.804          6        640: 100%|██████████| 25/25 [01:24<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:09<0\n",
      "                   all         36         40      0.556      0.505      0.476      0.204\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/20         0G      1.306      2.711      1.646          5        640: 100%|██████████| 25/25 [01:32<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:10<0\n",
      "                   all         36         40      0.537      0.578      0.553      0.284\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/20         0G      1.187      2.634      1.626          4        640: 100%|██████████| 25/25 [01:25<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:13<0\n",
      "                   all         36         40      0.599      0.598      0.576      0.333\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/20         0G      1.215      2.431      1.655          8        640: 100%|██████████| 25/25 [01:28<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:10<0\n",
      "                   all         36         40      0.544      0.539      0.586      0.375\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/20         0G       1.16      2.357      1.574          6        640: 100%|██████████| 25/25 [01:25<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:10<0\n",
      "                   all         36         40      0.588      0.629      0.621       0.37\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/20         0G      1.177      2.286      1.558          5        640: 100%|██████████| 25/25 [01:28<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:09<0\n",
      "                   all         36         40      0.485      0.734      0.613      0.362\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/20         0G      1.173      2.282      1.516          5        640: 100%|██████████| 25/25 [01:28<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:10<0\n",
      "                   all         36         40      0.515      0.769      0.653      0.394\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/20         0G      1.098       2.21      1.521          5        640: 100%|██████████| 25/25 [01:27<00:00,  3.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:10<0\n",
      "                   all         36         40      0.542      0.701      0.661      0.393\n",
      "\n",
      "20 epochs completed in 0.560 hours.\n",
      "Optimizer stripped from runs\\detect\\train18\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train18\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train18\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.171  Python-3.11.5 torch-2.0.1+cpu CPU (Intel Core(TM) i5-10300H 2.50GHz)\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:09<0\n",
      "                   all         36         40       0.54      0.701      0.661      0.393\n",
      "                helmet         36         11      0.522      0.727      0.697      0.454\n",
      "                minion         36         14      0.569      0.643      0.617      0.357\n",
      "              umbrella         36         15       0.53      0.733      0.668      0.368\n",
      "Speed: 3.7ms preprocess, 235.4ms inference, 0.0ms loss, 4.8ms postprocess per image\n",
      "Ultralytics YOLOv8.0.171  Python-3.11.5 torch-2.0.1+cpu CPU (Intel Core(TM) i5-10300H 2.50GHz)\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python codes\\Learn\\OpenCV\\Interlinked\\Interlinked.v1i.yolov8\\valid\\labels.cache... 36 images, 0 backgr\u001b[0m\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 25, len(boxes) = 40. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:09<0\n",
      "                   all         36         40       0.54      0.701      0.661      0.393\n",
      "                helmet         36         11      0.522      0.727      0.697      0.454\n",
      "                minion         36         14      0.569      0.643      0.617      0.357\n",
      "              umbrella         36         15       0.53      0.733      0.668      0.368\n",
      "Speed: 4.3ms preprocess, 231.9ms inference, 0.0ms loss, 4.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO \n",
    "import cv2\n",
    "# Load a model \n",
    "model = YOLO(\"yolov8n.pt\") \n",
    "# load a pretrained model \n",
    "# Use the model \n",
    "results = model.train(data=\"E:\\Python codes\\Learn\\OpenCV\\Interlinked\\Interlinked.v1i.yolov8\\data1.yaml\", epochs=20,batch=5) # train the model\n",
    "#results = model.train(data=\"coco128.yaml\", epochs=1)\n",
    "#$results = model.predict(data=\"config.yaml\")\n",
    "# results = model.val() # evaluate model performance on the validation data set 0\n",
    "results = model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575e79e-cdf0-42b1-bff8-491b4a44047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open the webcam.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to read frame from the webcam.\")\n",
    "        break\n",
    "\n",
    "    # Perform object detection on the frame\n",
    "    results = model(frame)\n",
    "    \n",
    "    # Display the frame with object detection results\n",
    "    \n",
    "  \n",
    "    cv2.imshow('Object Detection', frame)\n",
    "    # Press 'q' to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()# predict on an image\n",
    "# success = YOLO(\"yolov8n.pt\").export(format=\"onnx\") # export a model to ONNXqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c61bbc-1b2b-481f-9263-aa4e0253c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad0bf6e-15bc-48b9-9751-089003be35ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
